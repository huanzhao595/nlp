{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad7e71ef-823a-4844-93d6-8cc0b05e01df",
   "metadata": {},
   "source": [
    "## Step 1: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82a56503-3045-414b-9338-35e66b958e3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T22:26:20.903930Z",
     "iopub.status.busy": "2024-11-14T22:26:20.902838Z",
     "iopub.status.idle": "2024-11-14T22:26:24.387849Z",
     "shell.execute_reply": "2024-11-14T22:26:24.386565Z",
     "shell.execute_reply.started": "2024-11-14T22:26:20.903888Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /usr/local/lib/python3.8/dist-packages (1.6.17)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.8/dist-packages (from kaggle) (6.0.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.8/dist-packages (from kaggle) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: python-slugify in /usr/local/lib/python3.8/dist-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from kaggle) (2.29.0)\n",
      "Requirement already satisfied: six>=1.10 in /usr/lib/python3/dist-packages (from kaggle) (1.14.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from kaggle) (4.65.0)\n",
      "Requirement already satisfied: urllib3 in /usr/lib/python3/dist-packages (from kaggle) (1.25.8)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.8/dist-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.8/dist-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->kaggle) (2.8)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#1.1\n",
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4495ed93-1889-4c7d-9965-d71501b302df",
   "metadata": {},
   "source": [
    "1.2: Configure Kaggle Credentials\n",
    "Create a .kaggle folder and move the Kaggle API credentials file (kaggle.json) into it.\n",
    "Adjust the file permissions, then use the command-line tool to download the specified dataset.\n",
    "Configure Kaggle API credentials and download the target dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fea4365-f235-4328-8c7d-6ef277954b92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T22:26:24.390468Z",
     "iopub.status.busy": "2024-11-14T22:26:24.389749Z",
     "iopub.status.idle": "2024-11-14T22:26:24.394870Z",
     "shell.execute_reply": "2024-11-14T22:26:24.393721Z",
     "shell.execute_reply.started": "2024-11-14T22:26:24.390439Z"
    }
   },
   "outputs": [],
   "source": [
    "#1.2\n",
    "#!mkdir -p ~/.kaggle\n",
    "#!mv kaggle.json ~/.kaggle/\n",
    "#!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "#!kaggle datasets download -d chadgostopp/recsys-challenge-2015 --file yoochoose-clicks.dat\n",
    "\n",
    "#!unzip yoochoose-clicks.dat.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0daf18-e7ef-44a6-ba02-a5bc64e8c3f9",
   "metadata": {},
   "source": [
    "1.3: Import Dependencies\n",
    "Import the required libraries, \n",
    "including tools for data processing, \n",
    "GPU acceleration, and specific recommender system utilities.\n",
    "Provide the necessary tools for data loading, processing, \n",
    "and building the recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12b45647-331d-448a-8701-dcfb4b39572c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T23:02:29.951646Z",
     "iopub.status.busy": "2024-12-01T23:02:29.951308Z",
     "iopub.status.idle": "2024-12-01T23:02:34.318538Z",
     "shell.execute_reply": "2024-12-01T23:02:34.317495Z",
     "shell.execute_reply.started": "2024-12-01T23:02:29.951620Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/merlin/dtypes/mappings/tf.py:52: UserWarning: Tensorflow dtype mappings did not load successfully due to an error: No module named 'tensorflow'\n",
      "  warn(f\"Tensorflow dtype mappings did not load successfully due to an error: {exc.msg}\")\n",
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#1.3\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import calendar\n",
    "import datetime\n",
    "\n",
    "import cudf\n",
    "import cupy\n",
    "import nvtabular as nvt\n",
    "from merlin.dag import ColumnSelector\n",
    "from merlin.schema import Schema, Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94882fd3-83bf-48ac-a04c-85e59a5c0396",
   "metadata": {},
   "source": [
    "1.4: Disable CUDA Occupancy Warnings\n",
    "Resolve warnings caused by insufficient GPU occupancy to ensure a stable CUDA runtime environment.\n",
    "Disable low occupancy warnings to prevent interference with log output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83f82898-0962-4cca-82f0-e887c3c79ca8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T23:02:38.235903Z",
     "iopub.status.busy": "2024-12-01T23:02:38.235197Z",
     "iopub.status.idle": "2024-12-01T23:02:38.239858Z",
     "shell.execute_reply": "2024-12-01T23:02:38.238944Z",
     "shell.execute_reply.started": "2024-12-01T23:02:38.235863Z"
    }
   },
   "outputs": [],
   "source": [
    "#1.4\n",
    "#Avoid Numba low occupancy warnings:\n",
    "from numba import config\n",
    "config.CUDA_LOW_OCCUPANCY_WARNINGS = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f84578-946f-4366-83b1-362f8abdda97",
   "metadata": {},
   "source": [
    "## Step 2: Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f435d6-2b07-448d-a30f-cd0993af178c",
   "metadata": {},
   "source": [
    "2.1 Load Raw Data\n",
    "Define input and output data paths, as well as a flag indicating whether to use synthetic datasets.\n",
    "Prepare directories and file paths for loading and saving data in subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8588ba31-240f-472f-89bb-3d8e4f55aebb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T23:02:41.779932Z",
     "iopub.status.busy": "2024-12-01T23:02:41.779621Z",
     "iopub.status.idle": "2024-12-01T23:02:41.784826Z",
     "shell.execute_reply": "2024-12-01T23:02:41.783722Z",
     "shell.execute_reply.started": "2024-12-01T23:02:41.779908Z"
    }
   },
   "outputs": [],
   "source": [
    "#2.1\n",
    "# Define Data Input and Output Paths\n",
    "DATA_FOLDER = \"data\"  # Specify the data folder directly\n",
    "FILENAME_PATTERN = 'yoochoose-clicks.dat'  # File name pattern\n",
    "DATA_PATH = os.path.join(DATA_FOLDER, FILENAME_PATTERN)  # Combine full data path\n",
    "\n",
    "OUTPUT_FOLDER = \"transformed_data\"  # Output folder path\n",
    "OVERWRITE = False  # Flag to determine whether to overwrite existing files\n",
    "\n",
    "USE_SYNTHETIC = False  # Flag to indicate whether to use a synthetic dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3167a43f-748f-4a2d-b32b-6fa21cc0b52a",
   "metadata": {},
   "source": [
    "2.2 Generate Synthetic User-Item Interaction Dataset\n",
    "Define a function to generate synthetic clickstream datasets based on random distributions.\n",
    "Create simulated clickstream data, including session, item, and category features. The synthetic dataset mimics user interactions with items within a specified time range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06b9f45d-eaff-4ca3-89b3-6448077bb3bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T23:19:53.090817Z",
     "iopub.status.busy": "2024-12-01T23:19:53.090423Z",
     "iopub.status.idle": "2024-12-01T23:19:53.577997Z",
     "shell.execute_reply": "2024-12-01T23:19:53.577109Z",
     "shell.execute_reply.started": "2024-12-01T23:19:53.090787Z"
    }
   },
   "outputs": [],
   "source": [
    "#2.2\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import calendar\n",
    "\n",
    "def generate_synthetic_data(\n",
    "    start_date: datetime.date, end_date: datetime.date, rows_per_day: int = 10000\n",
    ") -> pd.DataFrame:\n",
    "    # Ensure end_date is later than start_date\n",
    "    assert end_date > start_date, \"end_date must be later than start_date\"\n",
    "\n",
    "    # Calculate the number of days in the range\n",
    "    number_of_days = (end_date - start_date).days\n",
    "    # Calculate total number of rows\n",
    "    total_number_of_rows = number_of_days * rows_per_day\n",
    "\n",
    "    # Generate a long-tail distribution of item interactions to simulate varying item popularity\n",
    "    long_tailed_item_distribution = np.clip(\n",
    "        np.random.lognormal(mean=3.0, sigma=1.0, size=total_number_of_rows).astype(np.int64), 1, 50000\n",
    "    )\n",
    "\n",
    "    # Generate random session and item interaction features\n",
    "    df = pd.DataFrame({\n",
    "        \"session_id\": np.random.randint(low=70000, high=80000, size=total_number_of_rows),  # Session ID\n",
    "        \"item_id\": long_tailed_item_distribution  # Item ID\n",
    "    })\n",
    "\n",
    "    # Generate category mapping for each item_id\n",
    "    df[\"category\"] = pd.cut(df[\"item_id\"], bins=334, labels=np.arange(1, 335)).astype(np.int64)\n",
    "\n",
    "    max_session_length = 60 * 60  # Maximum session length in seconds (1 hour)\n",
    "\n",
    "    # Define function to add timestamps to sessions\n",
    "    def add_timestamp_to_session(session: pd.DataFrame):\n",
    "        # Randomly generate start date and time for a session\n",
    "        random_start_date_and_time = calendar.timegm(\n",
    "            (\n",
    "                start_date +\n",
    "                datetime.timedelta(days=np.random.randint(0, number_of_days)) +  # Add day offset\n",
    "                datetime.timedelta(seconds=np.random.randint(0, 86_400))  # Add time offset within the day\n",
    "            ).timetuple()\n",
    "        )\n",
    "        # Generate timestamps for each interaction in the session\n",
    "        session[\"timestamp\"] = random_start_date_and_time + np.clip(\n",
    "            np.random.lognormal(mean=3.0, sigma=1.0, size=len(session)).astype(np.int64),\n",
    "            0, max_session_length\n",
    "        )\n",
    "        return session\n",
    "\n",
    "    # Apply timestamp generation for each session\n",
    "    df = df.groupby(\"session_id\").apply(add_timestamp_to_session).reset_index(drop=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41253892-82b5-4c9e-98db-d9fd82d124d0",
   "metadata": {},
   "source": [
    "2.3: Load and Clean Data\n",
    "Load data files (real or synthetic) and sort them by session_id and timestamp.\n",
    "Load the data and ensure it is ordered correctly, establishing a foundation for subsequent processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d457cfe8-7157-42e1-b076-2ddf4ef609b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T23:23:33.068367Z",
     "iopub.status.busy": "2024-12-01T23:23:33.067887Z",
     "iopub.status.idle": "2024-12-01T23:23:41.786881Z",
     "shell.execute_reply": "2024-12-01T23:23:41.786031Z",
     "shell.execute_reply.started": "2024-12-01T23:23:33.068339Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>item_id</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2014-04-07T10:51:09.277Z</td>\n",
       "      <td>214536502</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2014-04-07T10:54:09.868Z</td>\n",
       "      <td>214536500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2014-04-07T10:54:46.998Z</td>\n",
       "      <td>214536506</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2014-04-07T10:57:00.306Z</td>\n",
       "      <td>214577561</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2014-04-07T13:56:37.614Z</td>\n",
       "      <td>214662742</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_id                 timestamp    item_id category\n",
       "0           1  2014-04-07T10:51:09.277Z  214536502        0\n",
       "1           1  2014-04-07T10:54:09.868Z  214536500        0\n",
       "2           1  2014-04-07T10:54:46.998Z  214536506        0\n",
       "3           1  2014-04-07T10:57:00.306Z  214577561        0\n",
       "4           2  2014-04-07T13:56:37.614Z  214662742        0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2.3\n",
    "import os\n",
    "import datetime\n",
    "import cudf\n",
    "\n",
    "# Ensure the 'data' directory exists\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "\n",
    "# Check if the file exists in the data directory\n",
    "file_path = 'yoochoose-clicks.dat'\n",
    "\n",
    "# Determine data source based on the USE_SYNTHETIC environment variable\n",
    "USE_SYNTHETIC = os.environ.get(\"USE_SYNTHETIC\", \"False\") == \"True\"  \n",
    "# Set USE_SYNTHETIC flag from environment or default to False\n",
    "\n",
    "if USE_SYNTHETIC:\n",
    "    # Retrieve date range from environment variables or use defaults, format: 'YYYY/MM/DD'\n",
    "    # Synthetic data generation logic\n",
    "    START_DATE = os.environ.get(\"START_DATE\", \"2014/4/1\")\n",
    "    END_DATE = os.environ.get(\"END_DATE\", \"2014/4/5\")\n",
    "    \n",
    "    # Call the generate_synthetic_data function to create synthetic data\n",
    "    interactions_df = generate_synthetic_data(\n",
    "        datetime.datetime.strptime(START_DATE, '%Y/%m/%d'),\n",
    "        datetime.datetime.strptime(END_DATE, '%Y/%m/%d')\n",
    "    )\n",
    "    \n",
    "    # Convert synthetic data from pandas DataFrame to cudf DataFrame\n",
    "    interactions_df = cudf.from_pandas(interactions_df)\n",
    "    \n",
    "# Real data loading logic\n",
    "else:\n",
    "    # If the file exists, read it using cudf\n",
    "    column_names = ['session_id', 'timestamp', 'item_id', 'category']  # Assume these are the column names\n",
    "    interactions_merged_df = cudf.read_csv('yoochoose-clicks.dat', names=column_names, header=None)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "interactions_merged_df.head()  # Use interactions_merged_df instead of interactions_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d036df-4f3a-49fe-9997-47f4450fee28",
   "metadata": {},
   "source": [
    "2.4: Data Deduplication\n",
    "Remove duplicate item interactions within the same session to avoid data redundancy.\n",
    "Eliminate duplicate interactions to improve the efficiency of model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b8ce316-e081-46fd-a448-2381d7d41b94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T23:25:57.252379Z",
     "iopub.status.busy": "2024-12-01T23:25:57.251721Z",
     "iopub.status.idle": "2024-12-01T23:26:04.202819Z",
     "shell.execute_reply": "2024-12-01T23:26:04.201501Z",
     "shell.execute_reply.started": "2024-12-01T23:25:57.252343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count after removing in-session repeated interactions: 28971543\n"
     ]
    }
   ],
   "source": [
    "#2.4\n",
    "import os\n",
    "import datetime\n",
    "import cudf\n",
    "\n",
    "# Set the USE_SYNTHETIC variable (based on environment variable or directly defined)\n",
    "USE_SYNTHETIC = os.environ.get(\"USE_SYNTHETIC\", \"False\") == \"True\"\n",
    "\n",
    "# Define the data file path\n",
    "file_path = 'yoochoose-clicks.dat'\n",
    "\n",
    "# Sort by timestamp, check the previous interaction, and remove duplicate interactions within the same session\n",
    "# Sort the DataFrame by session_id and timestamp, and create helper columns prev_item_id and prev_session_id\n",
    "# Filter duplicates, then drop the helper columns\n",
    "\n",
    "# If USE_SYNTHETIC is True, generate synthetic data; otherwise, load real data\n",
    "if USE_SYNTHETIC:\n",
    "    interactions_df = generate_synthetic_data(\n",
    "        start_date=datetime.datetime.strptime(START_DATE, '%Y/%m/%d'),\n",
    "        end_date=datetime.datetime.strptime(END_DATE, '%Y/%m/%d')\n",
    "    )\n",
    "    # Convert synthetic data from pandas DataFrame to cudf DataFrame\n",
    "    interactions_df = cudf.from_pandas(interactions_df)\n",
    "else:\n",
    "    # If the file exists, load real data\n",
    "    if os.path.exists(file_path):\n",
    "        interactions_df = cudf.read_csv(\n",
    "            file_path, sep=',', \n",
    "            names=['session_id','timestamp', 'item_id', 'category'], \n",
    "            dtype=['int', 'datetime64[s]', 'int', 'int']\n",
    "        )\n",
    "    else:\n",
    "        print(f\"File {file_path} not found!\")\n",
    "        # If the file does not exist, generate synthetic data\n",
    "        interactions_df = generate_synthetic_data(\n",
    "            datetime.datetime.strptime(\"2014/4/1\", '%Y/%m/%d'),\n",
    "            datetime.datetime.strptime(\"2014/4/5\", '%Y/%m/%d')\n",
    "        )\n",
    "        interactions_df = cudf.from_pandas(interactions_df)\n",
    "\n",
    "# Ensure the data is sorted by session_id and timestamp\n",
    "interactions_df = interactions_df.sort_values(['session_id', 'timestamp'])\n",
    "\n",
    "# Create new columns prev_item_id and prev_session_id for comparison\n",
    "interactions_df['prev_item_id'] = interactions_df['item_id'].shift(1)\n",
    "interactions_df['prev_session_id'] = interactions_df['session_id'].shift(1)\n",
    "\n",
    "# Filter out duplicate interactions within the same session\n",
    "interactions_df = interactions_df[\n",
    "    ~((interactions_df['session_id'] == interactions_df['prev_session_id']) & \n",
    "       (interactions_df['item_id'] == interactions_df['prev_item_id']))\n",
    "]\n",
    "\n",
    "# Drop helper columns\n",
    "interactions_df = interactions_df.drop(columns=['prev_item_id', 'prev_session_id'])\n",
    "\n",
    "# Output the count of data after removing duplicates\n",
    "print(\"Count after removing in-session repeated interactions: {}\".format(len(interactions_df)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eead765-9b73-42a2-95a7-d3a68e1ef7f7",
   "metadata": {},
   "source": [
    "## Step 3: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3a3cba-cf81-4874-adef-3811e258de0b",
   "metadata": {},
   "source": [
    "3.1: Generate Temporal Features\n",
    "Create a new feature for each item that indicates the time it first appeared.\n",
    "Generate item-related features to provide the model with additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23ab80ec-9743-40ea-bf45-013adfaa1d53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T23:33:30.864392Z",
     "iopub.status.busy": "2024-12-01T23:33:30.863721Z",
     "iopub.status.idle": "2024-12-01T23:33:31.426101Z",
     "shell.execute_reply": "2024-12-01T23:33:31.425116Z",
     "shell.execute_reply.started": "2024-12-01T23:33:30.864362Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   session_id                 timestamp    item_id category  \\\n",
      "0        1793  2014-04-02T10:54:58.414Z  214716980        0   \n",
      "1        2034  2014-04-06T10:14:56.353Z  214821275        0   \n",
      "2        2033  2014-04-07T15:58:45.078Z  214748338        0   \n",
      "3        2034  2014-04-06T10:32:18.768Z  214821275        0   \n",
      "4        2032  2014-04-05T23:23:07.289Z  214826728        0   \n",
      "\n",
      "            itemid_ts_first  \n",
      "0  2014-04-01T03:48:52.585Z  \n",
      "1  2014-04-01T03:24:08.036Z  \n",
      "2  2014-04-01T05:10:27.244Z  \n",
      "3  2014-04-01T03:24:08.036Z  \n",
      "4  2014-04-01T07:08:47.338Z  \n"
     ]
    }
   ],
   "source": [
    "#3.1\n",
    "# Create a new feature for \n",
    "#each item_id indicating the first time the item appeared (timestamp)\n",
    "\n",
    "# Create a DataFrame with the first timestamp for each item_id\n",
    "items_first_ts_df = interactions_merged_df.groupby('item_id')['timestamp'].min().reset_index()\n",
    "items_first_ts_df = items_first_ts_df.rename(columns={'timestamp': 'itemid_ts_first'})\n",
    "\n",
    "# Merge the new feature back to the original DataFrame\n",
    "interactions_merged_df = interactions_merged_df.merge(items_first_ts_df, on='item_id', how='left')\n",
    "\n",
    "# Display the result\n",
    "print(interactions_merged_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ca4f0a-412d-421e-b71a-3a68599a342b",
   "metadata": {},
   "source": [
    "3.2 Data Format Conversion and Saving\n",
    "Save the cleaned data as a Parquet file for use in subsequent steps.\n",
    "Persist the data to enhance the modularity of the processing workflow.\n",
    "\n",
    "** Parquet format allows for fast loading, cross-platform usage, compression, and partitioning, making it ideal for large-scale data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eacb8422-e372-4a5d-887b-e59403eb0ec8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T23:50:03.576044Z",
     "iopub.status.busy": "2024-12-01T23:50:03.574869Z",
     "iopub.status.idle": "2024-12-01T23:50:10.641328Z",
     "shell.execute_reply": "2024-12-01T23:50:10.640264Z",
     "shell.execute_reply.started": "2024-12-01T23:50:03.576012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique items: 52739\n"
     ]
    }
   ],
   "source": [
    "#3.2\n",
    "# Save the interactions_merged_df to disk to be able to use in the inference step.\n",
    "\n",
    "# Define the directory to save the file (use the existing 'data' folder in the root directory)\n",
    "DATA_FOLDER = \"/data\"\n",
    "\n",
    "# Ensure the directory exists before saving the file\n",
    "os.makedirs(DATA_FOLDER, exist_ok=True)\n",
    "\n",
    "# Save the DataFrame as a Parquet file\n",
    "output_file = os.path.join(DATA_FOLDER, 'interactions_merged_df.parquet')\n",
    "interactions_merged_df.to_parquet(output_file)\n",
    "\n",
    "# Print the number of unique items in the dataset\n",
    "unique_items_count = interactions_merged_df['item_id'].nunique()\n",
    "print(f\"Total unique items: {unique_items_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e59262-9506-491f-b381-ccbdca202499",
   "metadata": {},
   "source": [
    "3.3 Clear GPU Memory\n",
    "Release unused or temporary data during processing, such as raw or cleaned interaction data, session-related variables, and DataFrames storing the first appearance times of items.Release unused or temporary data during processing, such as raw or cleaned interaction data, session-related variables, and DataFrames storing the first appearance times of items.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "556ead2c-e209-470e-9405-c64f6ed70c14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T23:50:14.313026Z",
     "iopub.status.busy": "2024-12-01T23:50:14.312453Z",
     "iopub.status.idle": "2024-12-01T23:50:14.519836Z",
     "shell.execute_reply": "2024-12-01T23:50:14.518589Z",
     "shell.execute_reply.started": "2024-12-01T23:50:14.312994Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "815"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3.3\n",
    "# Free GPU memory\n",
    "\n",
    "# Delete only the variables that are defined in the current session\n",
    "if 'interactions_df' in globals(): \n",
    "    del interactions_df  # Stores raw or cleaned interaction data\n",
    "\n",
    "if 'session_past_ids' in globals():\n",
    "    del session_past_ids  # Possibly a feature variable related to sessions\n",
    "\n",
    "if 'items_first_ts_df' in globals():\n",
    "    del items_first_ts_df  # Stores the DataFrame containing the first appearance time of items\n",
    "\n",
    "# Run garbage collection\n",
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c829e0-fe61-43b9-91a3-b4d828997d09",
   "metadata": {},
   "source": [
    "## Step 4: Model Construction (Using NVTabular Libraries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d7dd97-7429-4720-9969-353fc721030e",
   "metadata": {},
   "source": [
    "4.1 Define Feature Engineering Workflow:\n",
    "This workflow includes steps such as categorical feature encoding, \n",
    "temporal feature construction, item recency calculation, \n",
    "and feature normalization to generate input features \n",
    "for the recommendation system and prepare data for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035666b1-0d17-4094-89d0-09e00961d1c1",
   "metadata": {},
   "source": [
    "1. ColumnSelector\n",
    "Functionality: Selects columns to define inputs for feature engineering.\n",
    "cat_feats = ColumnSelector(['category', 'item_id']) >> Categorify()\n",
    "session_ts = ColumnSelector(['timestamp'])\n",
    "\n",
    "2. Categorify\n",
    "Functionality: Encodes categorical features by mapping category variables to continuous integers.\n",
    "cat_feats = ColumnSelector(['category', 'item_id']) >> Categorify()\n",
    "\n",
    "3. LambdaOp\n",
    "Functionality: Applies custom functions to transform columns.\n",
    "session_time = session_ts >> LambdaOp(lambda col: cudf.to_datetime(col, unit='s'))\n",
    "\n",
    "4. Rename\n",
    "Functionality: Renames columns to make generated features more meaningful.\n",
    "session_time >> Rename(name='event_time_dt')\n",
    "\n",
    "5. LogOp\n",
    "Functionality: Applies a logarithmic transformation to continuous features, smoothing the distribution.\n",
    "recency_features >> LogOp()\n",
    "\n",
    "6. Normalize\n",
    "Functionality: Standardizes continuous features, outputting normalized values.\n",
    "recency_features >> Normalize(out_dtype=np.float32)\n",
    "\n",
    "7. stom Feature Operations\n",
    "Functionality: Allows the definition of complex feature engineering by inheriting from nvt.ops.Operator.\n",
    "class ItemRecency(nvt.ops.Operator):\n",
    "    def transform(self, columns, gdf):\n",
    "\n",
    "8. Workflow（in later cells）\n",
    "unctionality: Defines and executes a feature engineering workflow.\n",
    "workflow = nvt.Workflow(features)\n",
    "transformed_data = workflow.transform(input_dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05be2e97-136f-4210-916a-58e292f982a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T23:54:21.293088Z",
     "iopub.status.busy": "2024-12-01T23:54:21.292599Z",
     "iopub.status.idle": "2024-12-01T23:54:21.306247Z",
     "shell.execute_reply": "2024-12-01T23:54:21.305195Z",
     "shell.execute_reply.started": "2024-12-01T23:54:21.293052Z"
    }
   },
   "outputs": [],
   "source": [
    "#4.1\n",
    "import nvtabular as nvt\n",
    "import cudf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nvtabular import ColumnSelector\n",
    "from nvtabular.ops import Categorify, LambdaOp, Rename, LogOp, Normalize\n",
    "from merlin.schema import Schema\n",
    "\n",
    "\n",
    "# s1\n",
    "# Encode categorical features (category and item_id) as continuous integer indices.\n",
    "# This processing improves model training efficiency, especially for embedding layers.\n",
    "cat_feats = ColumnSelector(['category', 'item_id']) >> Categorify()\n",
    "\n",
    "# s2 Time feature construction\n",
    "# Create time features and Convert timestamps\n",
    "session_ts = ColumnSelector(['timestamp'])\n",
    "\n",
    "# Use LambdaOp to call cudf.to_datetime to convert second-level timestamps to datetime\n",
    "# Use Rename to rename the column to event_time_dt\n",
    "session_time = (\n",
    "    session_ts >> \n",
    "    LambdaOp(lambda col: cudf.to_datetime(col, unit='s')) >> \n",
    "    Rename(name='event_time_dt')\n",
    ")\n",
    "\n",
    "# Extract weekday information\n",
    "# Use LambdaOp to call col.dt.weekday, generating an integer value representing the day of the week\n",
    "sessiontime_weekday = (\n",
    "    session_time >> \n",
    "    LambdaOp(lambda col: col.dt.weekday) >> \n",
    "    Rename(name='et_dayofweek')\n",
    ")\n",
    "\n",
    "# Derive cyclical features: Define a custom lambda function\n",
    "def get_cycled_feature_value_sin(col, max_value):\n",
    "    value_scaled = (col + 0.000001) / max_value\n",
    "    value_sin = np.sin(2 * np.pi * value_scaled)\n",
    "    return value_sin\n",
    "\n",
    "# Use the custom function get_cycled_feature_value_sin to calculate sine values based on the periodic value\n",
    "weekday_sin = sessiontime_weekday >> (lambda col: get_cycled_feature_value_sin(col + 1, 7)) >> Rename(name='et_dayofweek_sin')\n",
    "\n",
    "# s3\n",
    "# Item recency calculation: the number of days between the first appearance of an item and the current interaction\n",
    "# Use itemid_ts_first (first appearance time of the item) to calculate the time difference for each interaction\n",
    "# Set negative values (invalid time differences) to 0\n",
    "class ItemRecency(nvt.ops.Operator):\n",
    "    def transform(self, columns, gdf):\n",
    "        for column in columns.names:\n",
    "            col = gdf[column]\n",
    "            item_first_timestamp = gdf['itemid_ts_first']\n",
    "            delta_days = (col - item_first_timestamp) / (60 * 60 * 24)\n",
    "            gdf[column + \"_age_days\"] = delta_days * (delta_days >= 0)  # Ensure no negative recency\n",
    "        return gdf\n",
    "\n",
    "    def compute_selector(\n",
    "        self,\n",
    "        input_schema: Schema,\n",
    "        selector: ColumnSelector,\n",
    "        parents_selector: ColumnSelector,\n",
    "        dependencies_selector: ColumnSelector,\n",
    "    ) -> ColumnSelector:\n",
    "        self._validate_matching_cols(input_schema, parents_selector, \"computing input selector\")\n",
    "        return parents_selector\n",
    "\n",
    "    def column_mapping(self, col_selector):\n",
    "        column_mapping = {}\n",
    "        for col_name in col_selector.names:\n",
    "            column_mapping[col_name + \"_age_days\"] = [col_name]\n",
    "        return column_mapping\n",
    "\n",
    "    @property\n",
    "    def dependencies(self):\n",
    "        return [\"itemid_ts_first\"]\n",
    "\n",
    "    @property\n",
    "    def output_dtype(self):\n",
    "        return np.float64\n",
    "\n",
    "recency_features = session_ts >> ItemRecency()\n",
    "\n",
    "# s4\n",
    "# Apply standardization to this continuous feature\n",
    "# Feature standardization helps accelerate model convergence and avoids training issues caused by varying feature scales.\n",
    "recency_features_norm = recency_features >> LogOp() >> Normalize(out_dtype=np.float32) >> Rename(name='product_recency_days_log_norm')\n",
    "\n",
    "# s5\n",
    "# Combine time-related features (event_time_dt, weekday, cyclical features, and item recency)\n",
    "time_features = (\n",
    "    session_time +\n",
    "    sessiontime_weekday +\n",
    "    weekday_sin + \n",
    "    recency_features_norm\n",
    ")\n",
    "\n",
    "# s6\n",
    "# Define the final feature set by combining categorical features and time features\n",
    "# Combine categorical features (category and item_id) with time features to form the final feature set\n",
    "# The feature set includes session information (session_id and timestamp), item categories, temporal information, and recency.\n",
    "features = ColumnSelector(['session_id', 'timestamp']) + cat_feats + time_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b314d5e-67d3-4391-9ff0-f02cc7fa450f",
   "metadata": {},
   "source": [
    "4.2 Groupby Operations and Feature Preprocessing Workflow Based on NVTabular\n",
    "Convert timestamp to datetime format and extract time-related features, \n",
    "such as hour and weekday. Perform aggregation, feature generation, \n",
    "and filtering operations on session-level data. \n",
    "Provide time-related context for the recommendation model, \n",
    "ultimately generating a feature set suitable for input to the recommendation system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b7852eb-52a1-4d02-a933-7bedfb596a8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T00:00:20.200481Z",
     "iopub.status.busy": "2024-12-02T00:00:20.199433Z",
     "iopub.status.idle": "2024-12-02T00:00:20.216426Z",
     "shell.execute_reply": "2024-12-02T00:00:20.215149Z",
     "shell.execute_reply.started": "2024-12-02T00:00:20.200440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Node Filter output>\n"
     ]
    }
   ],
   "source": [
    "#4.2 \n",
    "import nvtabular as nvt\n",
    "from nvtabular import ops\n",
    "from nvtabular.ops import ListSlice\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define column selector\n",
    "columns = nvt.ColumnSelector([\"session_id\", \"item_id\", \"category\", \"timestamp\", \"event_time_dt\", \"et_dayofweek_sin\", \"product_recency_days_log_norm\"])\n",
    "\n",
    "# Define Groupby Operator\n",
    "# Group session data by session_id to generate new aggregated features\n",
    "groupby_features = columns >> nvt.ops.Groupby(\n",
    "    groupby_cols=[\"session_id\"], \n",
    "    sort_cols=[\"timestamp\"],\n",
    "    aggs={\n",
    "        'item_id': [\"list\", \"count\"],\n",
    "        'category': [\"list\"],  \n",
    "        'timestamp': [\"first\"],\n",
    "        'event_time_dt': [\"first\"],\n",
    "        'et_dayofweek_sin': [\"list\"],\n",
    "        'product_recency_days_log_norm': [\"list\"]\n",
    "    },\n",
    "    name_sep=\"-\"\n",
    ")\n",
    "\n",
    "# Truncate sequence features to first 20 interacted items\n",
    "SESSIONS_MAX_LENGTH = 20\n",
    "\n",
    "# Tag items and continuous features\n",
    "item_feat = groupby_features[\"item_id-list\"] >> nvt.ops.TagAsItemID()\n",
    "cont_feats = groupby_features['et_dayofweek_sin-list', 'product_recency_days_log_norm-list'] >> nvt.ops.AddMetadata(tags=[\"CONTINUOUS\"])\n",
    "\n",
    "# Group features together\n",
    "groupby_features_list = item_feat + cont_feats + groupby_features['category-list']\n",
    "\n",
    "# Apply ListSlice to truncate sequences\n",
    "# Truncate sequence features (e.g., item list, continuous feature list) to a fixed length SESSIONS_MAX_LENGTH.\n",
    "groupby_features_truncated = groupby_features_list >> nvt.ops.ListSlice(-SESSIONS_MAX_LENGTH)\n",
    "\n",
    "# Calculate session day index based on 'event_time_dt-first' column\n",
    "day_index = (\n",
    "    groupby_features['event_time_dt-first'] >> \n",
    "    nvt.ops.LambdaOp(lambda col: (col - col.min()).dt.days + 1) >> \n",
    "    nvt.ops.Rename(f=lambda col: \"day_index\") >>\n",
    "    nvt.ops.AddMetadata(tags=[\"CATEGORICAL\"])\n",
    ")\n",
    "\n",
    "# Tag session_id column for serving with legacy API\n",
    "sess_id = groupby_features['session_id'] >> nvt.ops.AddMetadata(tags=[\"CATEGORICAL\"])\n",
    "\n",
    "# Select features for training\n",
    "selected_features = sess_id + groupby_features['item_id-count'] + groupby_features_truncated + day_index\n",
    "\n",
    "# Filter out sessions with fewer than 2 interactions to ensure data quality\n",
    "MINIMUM_SESSION_LENGTH = 2\n",
    "filtered_sessions = selected_features >> nvt.ops.Filter(f=lambda df: df[\"item_id-count\"] >= MINIMUM_SESSION_LENGTH)\n",
    "\n",
    "# Print the final processed result\n",
    "print(filtered_sessions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489fc13b-02a3-4f8b-a66d-94d8abfb65c6",
   "metadata": {},
   "source": [
    "## Step 5: Organize Feature Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fca5ab79-c69c-46aa-a4ea-59735889d445",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T02:24:26.913235Z",
     "iopub.status.busy": "2024-11-15T02:24:26.912755Z",
     "iopub.status.idle": "2024-11-15T02:24:26.928000Z",
     "shell.execute_reply": "2024-11-15T02:24:26.926777Z",
     "shell.execute_reply.started": "2024-11-15T02:24:26.913204Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'cudf.core.dataframe.DataFrame'>\n",
      "RangeIndex: 33003944 entries, 0 to 33003943\n",
      "Data columns (total 5 columns):\n",
      " #   Column           Dtype\n",
      "---  ------           -----\n",
      " 0   session_id       int64\n",
      " 1   timestamp        object\n",
      " 2   item_id          int64\n",
      " 3   category         object\n",
      " 4   itemid_ts_first  object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 2.4+ GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# test cell\n",
    "print(interactions_merged_df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03bb68d6-136e-4a5b-8036-c7a0b3faedb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T02:24:29.823929Z",
     "iopub.status.busy": "2024-11-15T02:24:29.823505Z",
     "iopub.status.idle": "2024-11-15T02:24:29.830070Z",
     "shell.execute_reply": "2024-11-15T02:24:29.828694Z",
     "shell.execute_reply.started": "2024-11-15T02:24:29.823904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['session_id', 'timestamp', 'item_id', 'category', 'itemid_ts_first'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# test cell\n",
    "print(interactions_merged_df.columns)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9052980-f900-4d83-95cd-1537f9917cd2",
   "metadata": {},
   "source": [
    "5.1: Data Preprocessing and Feature Construction\n",
    "A complete data preprocessing workflow is defined \n",
    "with the aim of generating the dataset required \n",
    "for feature engineering in a recommendation system. \n",
    "This process uses NVTabular and cuDF to carry out data preprocessing, \n",
    "feature engineering, metadata enhancement, and saving the output. \n",
    "The final output includes the processed data files along with \n",
    "the schema information of the features, generating more time-based and interaction-related features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17c31bbe-773c-4ebd-b8b0-54abc98b983f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T00:07:12.697416Z",
     "iopub.status.busy": "2024-12-02T00:07:12.696979Z",
     "iopub.status.idle": "2024-12-02T00:07:33.340872Z",
     "shell.execute_reply": "2024-12-02T00:07:33.339680Z",
     "shell.execute_reply.started": "2024-12-02T00:07:12.697387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            name                 tags  \\\n",
      "0                     session_id  (selected_features)   \n",
      "1                      timestamp  (selected_features)   \n",
      "2                        item_id  (selected_features)   \n",
      "3                       category  (selected_features)   \n",
      "4                  event_time_dt  (selected_features)   \n",
      "5               et_dayofweek_sin  (selected_features)   \n",
      "6  product_recency_days_log_norm  (selected_features)   \n",
      "\n",
      "                                               dtype  is_list  is_ragged  \\\n",
      "0  DType(name='int64', element_type=<ElementType....    False      False   \n",
      "1  DType(name='object', element_type=<ElementType...    False      False   \n",
      "2  DType(name='int64', element_type=<ElementType....    False      False   \n",
      "3  DType(name='object', element_type=<ElementType...    False      False   \n",
      "4  DType(name='datetime64[ns]', element_type=<Ele...    False      False   \n",
      "5  DType(name='float64', element_type=<ElementTyp...    False      False   \n",
      "6  DType(name='float64', element_type=<ElementTyp...    False      False   \n",
      "\n",
      "  properties.num_buckets  properties.freq_threshold  properties.max_size  \\\n",
      "0                   None                          0                    0   \n",
      "1                   None                          0                    0   \n",
      "2                   None                          0                    0   \n",
      "3                   None                          0                    0   \n",
      "4                   None                          0                    0   \n",
      "5                   None                          0                    0   \n",
      "6                   None                          0                    0   \n",
      "\n",
      "   properties.start_index properties.cat_path  properties.domain.min  \\\n",
      "0                       0                 NaN                      0   \n",
      "1                       0                 NaN                      0   \n",
      "2                       0                 NaN                      0   \n",
      "3                       0                 NaN                      0   \n",
      "4                       0                 NaN                      0   \n",
      "5                       0                 NaN                      0   \n",
      "6                       0                 NaN                      0   \n",
      "\n",
      "   properties.domain.max properties.domain.name  \\\n",
      "0                  52739                item_id   \n",
      "1                  52739                item_id   \n",
      "2                  52739                item_id   \n",
      "3                  52739                item_id   \n",
      "4                  52739                item_id   \n",
      "5                  52739                item_id   \n",
      "6                  52739                item_id   \n",
      "\n",
      "   properties.embedding_sizes.cardinality  \\\n",
      "0                                   52740   \n",
      "1                                   52740   \n",
      "2                                   52740   \n",
      "3                                   52740   \n",
      "4                                   52740   \n",
      "5                                   52740   \n",
      "6                                   52740   \n",
      "\n",
      "   properties.embedding_sizes.dimension properties.value_count.min  \\\n",
      "0                                   512                        NaN   \n",
      "1                                   512                        NaN   \n",
      "2                                   512                        NaN   \n",
      "3                                   512                        NaN   \n",
      "4                                   512                        NaN   \n",
      "5                                   512                        NaN   \n",
      "6                                   512                        NaN   \n",
      "\n",
      "  properties.value_count.max  \n",
      "0                        NaN  \n",
      "1                        NaN  \n",
      "2                        NaN  \n",
      "3                        NaN  \n",
      "4                        NaN  \n",
      "5                        NaN  \n",
      "6                        NaN  \n"
     ]
    }
   ],
   "source": [
    "#5.1 \n",
    "import cudf\n",
    "import numpy as np\n",
    "import nvtabular as nvt\n",
    "import os\n",
    "from datetime import datetime\n",
    "from nvtabular import ops\n",
    "\n",
    "# Define data loading\n",
    "# GPU-accelerated cudf provides efficient data loading capabilities, suitable for large-scale datasets\n",
    "column_names = ['session_id', 'timestamp', 'item_id', 'category']\n",
    "interactions_merged_df = cudf.read_csv('yoochoose-clicks.dat', names=column_names, header=None)\n",
    "\n",
    "# Time feature construction\n",
    "# Provide richer time information to capture the cyclical patterns of user behavior\n",
    "interactions_merged_df['event_time_dt'] = cudf.to_datetime(interactions_merged_df['timestamp'])\n",
    "interactions_merged_df['et_dayofweek_sin'] = np.sin(interactions_merged_df['event_time_dt'].dt.dayofweek * (2 * np.pi / 7))\n",
    "\n",
    "# Calculate the \"recency\" feature of products (recent activity feature of items)\n",
    "# Calculate the difference between the current time and the most recent interaction time of the item (in days).\n",
    "# Generate the product recency feature `product_recency_days_log_norm` by taking the logarithm to smooth the data distribution.\n",
    "current_time = datetime.now()\n",
    "current_time_cudf = cudf.to_datetime([current_time])[0]  # Use cudf.to_datetime\n",
    "time_difference = current_time_cudf - interactions_merged_df['event_time_dt']\n",
    "recency_days = time_difference.astype('float64') / (60 * 60 * 24 * 1e9)  # Convert nanoseconds to days\n",
    "interactions_merged_df['product_recency_days_log_norm'] = np.log(1 + recency_days)\n",
    "\n",
    "selected_features = ['session_id', 'timestamp', 'item_id', 'category', 'event_time_dt', 'et_dayofweek_sin', 'product_recency_days_log_norm']\n",
    "\n",
    "# Create a workflow node containing all columns\n",
    "# Define the feature set `selected_features`, including all generated features\n",
    "# Use AddMetadata to add more metadata information\n",
    "workflow_node = (\n",
    "    nvt.ColumnSelector(selected_features) \n",
    "    >> ops.AddMetadata(tags=[\"selected_features\"], properties={\n",
    "        \"num_buckets\": None,\n",
    "        \"freq_threshold\": 0,\n",
    "        \"max_size\": 0,\n",
    "        \"start_index\": 0,\n",
    "        \"domain\": {\"min\": 0, \"max\": 52739, \"name\": \"item_id\"},\n",
    "        \"embedding_sizes\": {\"cardinality\": 52740, \"dimension\": 512},\n",
    "        \"value_count\": {\"min\": 0, \"max\": 20}\n",
    "    })\n",
    ")\n",
    "\n",
    "# Data transformation and saving\n",
    "# Use NVTabular to define and execute the data preprocessing workflow\n",
    "workflow = nvt.Workflow(workflow_node)\n",
    "\n",
    "# Create a dataset object\n",
    "# Convert the loaded data to NVTabular's Dataset format for workflow processing\n",
    "dataset = nvt.Dataset(interactions_merged_df)\n",
    "\n",
    "# Learn feature statistics and transform the data, output to a Parquet file\n",
    "DATA_FOLDER = './processed_data'\n",
    "workflow.fit_transform(dataset).to_parquet(os.path.join(DATA_FOLDER, \"processed_nvt\"))\n",
    "\n",
    "# Print the output schema to observe detailed information of each column (output and processed schema)\n",
    "output_schema = workflow.output_schema\n",
    "output_list = []\n",
    "\n",
    "# Format the output\n",
    "for col in output_schema.column_schemas.values():\n",
    "    output_list.append({\n",
    "        'name': col.name,\n",
    "        'tags': col.tags,\n",
    "        'dtype': col.dtype,\n",
    "        'is_list': col.is_list,\n",
    "        'is_ragged': col.is_ragged,\n",
    "        'properties.num_buckets': col.properties.get('num_buckets', 'NaN'),\n",
    "        'properties.freq_threshold': col.properties.get('freq_threshold', 'NaN'),\n",
    "        'properties.max_size': col.properties.get('max_size', 'NaN'),\n",
    "        'properties.start_index': col.properties.get('start_index', 'NaN'),\n",
    "        'properties.cat_path': col.properties.get('cat_path', 'NaN'),\n",
    "        'properties.domain.min': col.properties.get('domain', {}).get('min', 'NaN'),\n",
    "        'properties.domain.max': col.properties.get('domain', {}).get('max', 'NaN'),\n",
    "        'properties.domain.name': col.properties.get('domain', {}).get('name', 'NaN'),\n",
    "        'properties.embedding_sizes.cardinality': col.properties.get('embedding_sizes', {}).get('cardinality', 'NaN'),\n",
    "        'properties.embedding_sizes.dimension': col.properties.get('embedding_sizes', {}).get('dimension', 'NaN'),\n",
    "        'properties.value_count.min': col.properties.get('value_count', {}).get('min', 'NaN'),\n",
    "        'properties.value_count.max': col.properties.get('value_count', {}).get('max', 'NaN')\n",
    "    })\n",
    "\n",
    "# Finally, convert the result to a Pandas DataFrame and print the output.\n",
    "import pandas as pd\n",
    "output_df = pd.DataFrame(output_list)\n",
    "print(output_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249a766e-00bd-4468-b36a-11587e88bd95",
   "metadata": {},
   "source": [
    "5.2 NVTabular Workflow Saved to Disk for Reuse\n",
    "\n",
    "The NVTabular workflow is saved to disk \n",
    "so that it can be reused later without needing to \n",
    "redefine or retrain the statistical information \n",
    "(such as categorical feature dictionary mappings, means, standard deviations, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d1a0270-0909-4eb4-9e90-47c19f30251b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T00:22:14.847077Z",
     "iopub.status.busy": "2024-12-02T00:22:14.846628Z",
     "iopub.status.idle": "2024-12-02T00:22:14.866844Z",
     "shell.execute_reply": "2024-12-02T00:22:14.866046Z",
     "shell.execute_reply.started": "2024-12-02T00:22:14.847042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow saved at: ./processed_data/workflow_etl\n"
     ]
    }
   ],
   "source": [
    "#5.2\n",
    "\n",
    "# Save the workflow\n",
    "# Use workflow.save() to save the workflow to the specified path for later loading and reuse\n",
    "WORKFLOW_SAVE_PATH = os.path.join(DATA_FOLDER, \"workflow_etl\")\n",
    "workflow.save(WORKFLOW_SAVE_PATH)\n",
    "\n",
    "# Inform the user where the workflow has been saved\n",
    "print(f\"Workflow saved at: {WORKFLOW_SAVE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba539ad3-23d3-40fd-a14d-ff6f24e6cc86",
   "metadata": {},
   "source": [
    "## Step 6: Session-Level Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ea749a-dfa3-4d34-bb09-b312b2aff556",
   "metadata": {},
   "source": [
    "6.1 Process Session-Level Data\n",
    "\n",
    "Process a session-level dataset by splitting it \n",
    "into multiple subsets based on days. Further, split each subset into training, \n",
    "validation, and test sets. Finally, save these datasets as separate Parquet files.\n",
    "\n",
    "Split the processed data by day and aggregate each session to \n",
    "generate sequence features. Focus on item interaction records, \n",
    "time features, and categorical features for each session.\n",
    "\n",
    "Determine feature types, including categorical features, continuous features, and target labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3a7416c-7371-4bef-804c-b6464acd6ebf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T21:03:25.131705Z",
     "iopub.status.busy": "2024-11-26T21:03:25.131354Z",
     "iopub.status.idle": "2024-11-26T21:03:27.574915Z",
     "shell.execute_reply": "2024-11-26T21:03:27.574023Z",
     "shell.execute_reply.started": "2024-11-26T21:03:25.131678Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   session_id               timestamp    item_id category  \\\n",
      "0           1 2014-04-07 10:51:09.277  214536502        0   \n",
      "1           1 2014-04-07 10:54:09.868  214536500        0   \n",
      "2           1 2014-04-07 10:54:46.998  214536506        0   \n",
      "\n",
      "            event_time_dt  et_dayofweek_sin  product_recency_days_log_norm  \\\n",
      "0 2014-04-07 10:51:09.277               0.0                       8.265496   \n",
      "1 2014-04-07 10:54:09.868               0.0                       8.265496   \n",
      "2 2014-04-07 10:54:46.998               0.0                       8.265496   \n",
      "\n",
      "   day_index  \n",
      "0      16167  \n",
      "1      16167  \n",
      "2      16167  \n",
      "Data for day 16339 exported to folder: ./exported_by_day/day_16339\n",
      "Data for day 16340 exported to folder: ./exported_by_day/day_16340\n",
      "Data for day 16341 exported to folder: ./exported_by_day/day_16341\n",
      "Data for day 16342 exported to folder: ./exported_by_day/day_16342\n",
      "Data for day 16343 exported to folder: ./exported_by_day/day_16343\n",
      "All data successfully exported to ./exported_by_day\n"
     ]
    }
   ],
   "source": [
    "#6.1\n",
    "import cudf\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Export data split by day\n",
    "# Define dataset path\n",
    "DATA_FOLDER = './processed_data'\n",
    "\n",
    "# Read in the processed train dataset (load and filter data)\n",
    "# Read in the processed train dataset\n",
    "sessions_gdf = cudf.read_parquet(os.path.join(DATA_FOLDER, \"processed_nvt/part_0.parquet\"))\n",
    "\n",
    "# Ensure the 'day_index' column exists, calculate 'day_index' based on timestamp\n",
    "if 'day_index' not in sessions_gdf.columns:\n",
    "    # Assuming 'timestamp' is a string representing a date, convert to datetime\n",
    "    sessions_gdf['timestamp'] = cudf.to_datetime(sessions_gdf['timestamp'])\n",
    "    epoch = datetime(1970, 1, 1)\n",
    "    sessions_gdf['day_index'] = (sessions_gdf['timestamp'] - epoch).dt.days\n",
    "\n",
    "# If using a synthetic dataset, filter based on environment variable THRESHOLD_DAY_INDEX\n",
    "USE_SYNTHETIC = os.environ.get(\"USE_SYNTHETIC\", \"False\") == \"True\"\n",
    "if USE_SYNTHETIC:\n",
    "    THRESHOLD_DAY_INDEX = int(os.environ.get(\"THRESHOLD_DAY_INDEX\", '1'))\n",
    "    sessions_gdf = sessions_gdf[sessions_gdf['day_index'] >= THRESHOLD_DAY_INDEX]\n",
    "else:\n",
    "    sessions_gdf = sessions_gdf[sessions_gdf['day_index'] >= 178]\n",
    "\n",
    "# Print the first few rows of the processed data to confirm data loading and filtering\n",
    "print(sessions_gdf.head(3))\n",
    "\n",
    "# Perform session-level aggregation (aggregate by session)\n",
    "# Grouping by 'session_id' to get list features for each session\n",
    "grouped_sessions_gdf = sessions_gdf.groupby('session_id').agg({\n",
    "    'item_id': list,\n",
    "    'et_dayofweek_sin': list,\n",
    "    'product_recency_days_log_norm': list,\n",
    "    'category': list,\n",
    "    'day_index': 'first'  # We take the first 'day_index' as the session level day_index\n",
    "})\n",
    "\n",
    "# Rename columns to match original output format\n",
    "grouped_sessions_gdf = grouped_sessions_gdf.rename(columns={\n",
    "    'item_id': 'item_id-list',\n",
    "    'et_dayofweek_sin': 'et_dayofweek_sin-list',\n",
    "    'product_recency_days_log_norm': 'product_recency_days_log_norm-list',\n",
    "    'category': 'category-list'\n",
    "})\n",
    "\n",
    "# We are only interested in the last 5 days\n",
    "LAST_N_DAYS = 5\n",
    "unique_days = sorted(grouped_sessions_gdf['day_index'].unique().to_pandas())[-LAST_N_DAYS:]\n",
    "\n",
    "# Define output directory\n",
    "EXPORT_FOLDER = './exported_by_day'\n",
    "if not os.path.exists(EXPORT_FOLDER):\n",
    "    os.makedirs(EXPORT_FOLDER)\n",
    "\n",
    "# Create folders by day and split dataset\n",
    "for day in unique_days:\n",
    "    day_data = grouped_sessions_gdf[grouped_sessions_gdf['day_index'] == day]\n",
    "\n",
    "    # Split dataset into train, validation, and test sets (split data by day)\n",
    "    # Here we are arbitrarily splitting: 70% train, 15% validation, 15% test\n",
    "    train_size = int(len(day_data) * 0.7)\n",
    "    val_size = int(len(day_data) * 0.15)\n",
    "\n",
    "    train_data = day_data.iloc[:train_size]\n",
    "    val_data = day_data.iloc[train_size:train_size + val_size]\n",
    "    test_data = day_data.iloc[train_size + val_size:]\n",
    "\n",
    "    # Create a separate folder for each day\n",
    "    day_folder = os.path.join(EXPORT_FOLDER, f'day_{day}')\n",
    "    if not os.path.exists(day_folder):\n",
    "        os.makedirs(day_folder)\n",
    "\n",
    "    # Save data to Parquet files\n",
    "    train_data.to_parquet(os.path.join(day_folder, \"train.parquet\"))\n",
    "    val_data.to_parquet(os.path.join(day_folder, \"validation.parquet\"))\n",
    "    test_data.to_parquet(os.path.join(day_folder, \"test.parquet\"))\n",
    "\n",
    "    print(f\"Data for day {day} exported to folder: {day_folder}\")\n",
    "\n",
    "# Inform the user that the data has been exported\n",
    "print(f\"All data successfully exported to {EXPORT_FOLDER}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f153806c-a3e2-4f9b-90a2-51995ab0ba48",
   "metadata": {},
   "source": [
    "6.2: Simplify Data Splitting\n",
    "Use the save_time_based_splits function to split the dataset by time \n",
    "(based on timestamps and partition columns) \n",
    "and save it to the specified output directory. \n",
    "This simplifies the splitting and saving logic, \n",
    "relying on the efficient data processing capabilities of Transformers4Rec and NVTabular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3fcba93-2210-46be-951b-2f05d8cf12ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T00:22:53.605438Z",
     "iopub.status.busy": "2024-12-02T00:22:53.605042Z",
     "iopub.status.idle": "2024-12-02T00:23:32.031412Z",
     "shell.execute_reply": "2024-12-02T00:23:32.030282Z",
     "shell.execute_reply.started": "2024-12-02T00:22:53.605409Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating time-based splits: 100%|██████████| 183/183 [00:30<00:00,  6.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully split and saved to ./processed_data/preproc_sessions_by_day\n"
     ]
    }
   ],
   "source": [
    "#6.2\n",
    "from transformers4rec.utils.data_utils import save_time_based_splits\n",
    "import nvtabular as nvt\n",
    "import cudf\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Define dataset path\n",
    "DATA_FOLDER = './processed_data'\n",
    "\n",
    "# Read in the processed train dataset\n",
    "sessions_gdf = cudf.read_parquet(os.path.join(DATA_FOLDER, \"processed_nvt/part_0.parquet\"))\n",
    "\n",
    "# Ensure the 'day_index' column exists, calculate 'day_index' based on timestamp\n",
    "if 'day_index' not in sessions_gdf.columns:\n",
    "    # Assuming 'timestamp' is a string representing a date, convert to datetime\n",
    "    sessions_gdf['timestamp'] = cudf.to_datetime(sessions_gdf['timestamp'])\n",
    "    epoch = datetime(1970, 1, 1)\n",
    "    sessions_gdf['day_index'] = (sessions_gdf['timestamp'] - epoch).dt.days\n",
    "\n",
    "# Define the output folder\n",
    "OUTPUT_FOLDER = os.path.join(DATA_FOLDER, \"preproc_sessions_by_day\")\n",
    "if not os.path.exists(OUTPUT_FOLDER):\n",
    "    os.makedirs(OUTPUT_FOLDER)\n",
    "\n",
    "# Split and save by time\n",
    "save_time_based_splits(\n",
    "    data=nvt.Dataset(sessions_gdf),  # Input dataset\n",
    "    output_dir=OUTPUT_FOLDER,  # Output directory\n",
    "    partition_col='day_index',  # Column for splitting the data (split by day index)\n",
    "    timestamp_col='event_time_dt',  # Use event_time_dt as the timestamp column\n",
    ")\n",
    "\n",
    "print(f\"Data successfully split and saved to {OUTPUT_FOLDER}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304cc38d-0ade-4bc9-9ecb-6e3f60376470",
   "metadata": {},
   "source": [
    "6.3: Release GPU Memory\n",
    "Delete unnecessary variables and clean up GPU memory to free resources for subsequent processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "432905a6-2052-4acb-9f49-3376cbfcfa9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T00:25:09.509393Z",
     "iopub.status.busy": "2024-12-02T00:25:09.508929Z",
     "iopub.status.idle": "2024-12-02T00:25:09.852580Z",
     "shell.execute_reply": "2024-12-02T00:25:09.851243Z",
     "shell.execute_reply.started": "2024-12-02T00:25:09.509357Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory has been successfully released.\n"
     ]
    }
   ],
   "source": [
    "#6.3\n",
    "import gc\n",
    "import cupy as cp\n",
    "import rmm\n",
    "\n",
    "# Free GPU memory\n",
    "\n",
    "# Check if the variable exists, and delete only if it does\n",
    "if 'sessions_gdf' in locals() or 'sessions_gdf' in globals():\n",
    "    del sessions_gdf  # Delete session dataset to release GPU memory resources\n",
    "\n",
    "# Delete other potentially large datasets if they exist (optional)\n",
    "# if 'other_large_dataframe' in locals() or 'other_large_dataframe' in globals():\n",
    "#     del other_large_dataframe  # Delete other large datasets if necessary\n",
    "\n",
    "# Force Python's garbage collection to ensure Python-level memory is released\n",
    "gc.collect()  # Python's garbage collector can release CPU memory\n",
    "\n",
    "# Clean GPU memory\n",
    "cp.get_default_memory_pool().free_all_blocks()  # Use CuPy's memory pool to release GPU memory\n",
    "rmm.reinitialize()  # Reinitialize RMM to release any potentially locked GPU memory\n",
    "\n",
    "# Inform the user that memory has been released\n",
    "print(\"GPU memory has been successfully released.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8177828e-4706-4f4e-8038-b59648e8d2c5",
   "metadata": {},
   "source": [
    "## Step 7: Training Data Preparation and Model Fine-Tuning\n",
    "End-to-end session-based recommendations with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3136a101-13d5-42a7-ab85-958fd04ac8b0",
   "metadata": {},
   "source": [
    "7.1 Prepare Training Data\n",
    "Load a dataset in Parquet format, extract its schema information, \n",
    "and select specific features for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad6f17eb-6980-42c9-996c-2e3470d58597",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T00:31:37.250638Z",
     "iopub.status.busy": "2024-12-02T00:31:37.249971Z",
     "iopub.status.idle": "2024-12-02T00:31:37.554019Z",
     "shell.execute_reply": "2024-12-02T00:31:37.552879Z",
     "shell.execute_reply.started": "2024-12-02T00:31:37.250609Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>tags</th>\n",
       "      <th>dtype</th>\n",
       "      <th>is_list</th>\n",
       "      <th>is_ragged</th>\n",
       "      <th>properties.num_buckets</th>\n",
       "      <th>properties.freq_threshold</th>\n",
       "      <th>properties.max_size</th>\n",
       "      <th>properties.start_index</th>\n",
       "      <th>properties.cat_path</th>\n",
       "      <th>properties.embedding_sizes.cardinality</th>\n",
       "      <th>properties.embedding_sizes.dimension</th>\n",
       "      <th>properties.domain.min</th>\n",
       "      <th>properties.domain.max</th>\n",
       "      <th>properties.domain.name</th>\n",
       "      <th>properties.value_count.min</th>\n",
       "      <th>properties.value_count.max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>item_id-list</td>\n",
       "      <td>(Tags.ID, Tags.CATEGORICAL, Tags.LIST, Tags.IT...</td>\n",
       "      <td>DType(name='int64', element_type=&lt;ElementType....</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.item_id.parquet</td>\n",
       "      <td>52740.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52739.0</td>\n",
       "      <td>item_id</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>category-list</td>\n",
       "      <td>(Tags.LIST, Tags.CATEGORICAL)</td>\n",
       "      <td>DType(name='int64', element_type=&lt;ElementType....</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.category.parquet</td>\n",
       "      <td>335.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>334.0</td>\n",
       "      <td>category</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>product_recency_days_log_norm-list</td>\n",
       "      <td>(Tags.LIST, Tags.CONTINUOUS)</td>\n",
       "      <td>DType(name='float32', element_type=&lt;ElementTyp...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>et_dayofweek_sin-list</td>\n",
       "      <td>(Tags.LIST, Tags.CONTINUOUS)</td>\n",
       "      <td>DType(name='float64', element_type=&lt;ElementTyp...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "[{'name': 'item_id-list', 'tags': {<Tags.ID: 'id'>, <Tags.CATEGORICAL: 'categorical'>, <Tags.LIST: 'list'>, <Tags.ITEM_ID: 'item_id'>, <Tags.ITEM: 'item'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 0.0, 'cat_path': './/categories/unique.item_id.parquet', 'embedding_sizes': {'cardinality': 52740.0, 'dimension': 512.0}, 'domain': {'min': 0, 'max': 52739, 'name': 'item_id'}, 'value_count': {'min': 0, 'max': 20}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=20)))), 'is_list': True, 'is_ragged': True}, {'name': 'category-list', 'tags': {<Tags.LIST: 'list'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 0.0, 'cat_path': './/categories/unique.category.parquet', 'embedding_sizes': {'cardinality': 335.0, 'dimension': 42.0}, 'domain': {'min': 0, 'max': 334, 'name': 'category'}, 'value_count': {'min': 0, 'max': 20}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=20)))), 'is_list': True, 'is_ragged': True}, {'name': 'product_recency_days_log_norm-list', 'tags': {<Tags.LIST: 'list'>, <Tags.CONTINUOUS: 'continuous'>}, 'properties': {'value_count': {'min': 0, 'max': 20}}, 'dtype': DType(name='float32', element_type=<ElementType.Float: 'float'>, element_size=32, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=20)))), 'is_list': True, 'is_ragged': True}, {'name': 'et_dayofweek_sin-list', 'tags': {<Tags.LIST: 'list'>, <Tags.CONTINUOUS: 'continuous'>}, 'properties': {'value_count': {'min': 0, 'max': 20}}, 'dtype': DType(name='float64', element_type=<ElementType.Float: 'float'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=20)))), 'is_list': True, 'is_ragged': True}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#7.1\n",
    "# Import required libraries\n",
    "import os\n",
    "from merlin.schema import Schema\n",
    "from merlin.io import Dataset\n",
    "\n",
    "# Function: Load dataset\n",
    "def load_dataset(input_dir, file_name):\n",
    "    \"\"\"\n",
    "    Load parquet dataset\n",
    "    \"\"\"\n",
    "    return Dataset(os.path.join(input_dir, file_name))\n",
    "\n",
    "# Function: Extract schema and select specific features\n",
    "def get_schema(dataset, selected_features):\n",
    "    \"\"\"\n",
    "    Extract schema from the dataset and select features for model training\n",
    "    \"\"\"\n",
    "    schema = dataset.schema\n",
    "    return schema.select_by_name(selected_features)\n",
    "\n",
    "# Set environment variables and paths\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "INPUT_DATA_DIR = os.environ.get(\"INPUT_DATA_DIR\", \"\")\n",
    "OUTPUT_DIR = os.environ.get(\"OUTPUT_DIR\", f\"{INPUT_DATA_DIR}/preproc_sessions_by_day\")\n",
    "\n",
    "# Execute logic\n",
    "train_dataset = load_dataset(INPUT_DATA_DIR, \"processed_nvt/part_0.parquet\")\n",
    "selected_features = ['item_id-list', 'category-list', 'product_recency_days_log_norm-list', 'et_dayofweek_sin-list']\n",
    "schema = get_schema(train_dataset, selected_features)\n",
    "\n",
    "# Print schema information\n",
    "schema\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd073f87-3094-402e-9ba8-e40ab9fd1f89",
   "metadata": {},
   "source": [
    "7.2 Model Construction\n",
    "Create a session-based recommendation model based on XLNet, \n",
    "including input modules, prediction tasks, and Transformer configurations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b59580c6-c5bd-4eaf-be55-4b4ce45e604c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T00:31:43.724436Z",
     "iopub.status.busy": "2024-12-02T00:31:43.723315Z",
     "iopub.status.idle": "2024-12-02T00:31:43.942247Z",
     "shell.execute_reply": "2024-12-02T00:31:43.941196Z",
     "shell.execute_reply.started": "2024-12-02T00:31:43.724392Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Projecting inputs of NextItemPredictionTask to'64' As weight tying requires the input dimension '320' to be equal to the item-id embedding dimension '64'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (heads): ModuleList(\n",
       "    (0): Head(\n",
       "      (body): SequentialBlock(\n",
       "        (0): TabularSequenceFeatures(\n",
       "          (_aggregation): ConcatFeatures()\n",
       "          (to_merge): ModuleDict(\n",
       "            (continuous_module): SequentialBlock(\n",
       "              (0): ContinuousFeatures(\n",
       "                (filter_features): FilterFeatures()\n",
       "                (_aggregation): ConcatFeatures()\n",
       "              )\n",
       "              (1): SequentialBlock(\n",
       "                (0): DenseBlock(\n",
       "                  (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "                  (1): ReLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (2): AsTabular()\n",
       "            )\n",
       "            (categorical_module): SequenceEmbeddingFeatures(\n",
       "              (filter_features): FilterFeatures()\n",
       "              (embedding_tables): ModuleDict(\n",
       "                (item_id-list): Embedding(52740, 64, padding_idx=0)\n",
       "                (category-list): Embedding(335, 64, padding_idx=0)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (projection_module): SequentialBlock(\n",
       "            (0): DenseBlock(\n",
       "              (0): Linear(in_features=192, out_features=320, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (_masking): MaskedLanguageModeling()\n",
       "        )\n",
       "        (1): TansformerBlock(\n",
       "          (transformer): XLNetModel(\n",
       "            (word_embedding): Embedding(1, 320)\n",
       "            (layer): ModuleList(\n",
       "              (0): XLNetLayer(\n",
       "                (rel_attn): XLNetRelativeAttention(\n",
       "                  (layer_norm): LayerNorm((320,), eps=0.03, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.3, inplace=False)\n",
       "                )\n",
       "                (ff): XLNetFeedForward(\n",
       "                  (layer_norm): LayerNorm((320,), eps=0.03, elementwise_affine=True)\n",
       "                  (layer_1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "                  (layer_2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                  (dropout): Dropout(p=0.3, inplace=False)\n",
       "                )\n",
       "                (dropout): Dropout(p=0.3, inplace=False)\n",
       "              )\n",
       "              (1): XLNetLayer(\n",
       "                (rel_attn): XLNetRelativeAttention(\n",
       "                  (layer_norm): LayerNorm((320,), eps=0.03, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.3, inplace=False)\n",
       "                )\n",
       "                (ff): XLNetFeedForward(\n",
       "                  (layer_norm): LayerNorm((320,), eps=0.03, elementwise_affine=True)\n",
       "                  (layer_1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "                  (layer_2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                  (dropout): Dropout(p=0.3, inplace=False)\n",
       "                )\n",
       "                (dropout): Dropout(p=0.3, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (masking): MaskedLanguageModeling()\n",
       "        )\n",
       "      )\n",
       "      (prediction_task_dict): ModuleDict(\n",
       "        (next-item): NextItemPredictionTask(\n",
       "          (sequence_summary): SequenceSummary(\n",
       "            (summary): Identity()\n",
       "            (activation): Identity()\n",
       "            (first_dropout): Identity()\n",
       "            (last_dropout): Identity()\n",
       "          )\n",
       "          (metrics): ModuleList(\n",
       "            (0): NDCGAt()\n",
       "            (1): AvgPrecisionAt()\n",
       "            (2): RecallAt()\n",
       "          )\n",
       "          (loss): CrossEntropyLoss()\n",
       "          (embeddings): SequenceEmbeddingFeatures(\n",
       "            (filter_features): FilterFeatures()\n",
       "            (embedding_tables): ModuleDict(\n",
       "              (item_id-list): Embedding(52740, 64, padding_idx=0)\n",
       "              (category-list): Embedding(335, 64, padding_idx=0)\n",
       "            )\n",
       "          )\n",
       "          (item_embedding_table): Embedding(52740, 64, padding_idx=0)\n",
       "          (masking): MaskedLanguageModeling()\n",
       "          (task_block): SequentialBlock(\n",
       "            (0): DenseBlock(\n",
       "              (0): Linear(in_features=320, out_features=64, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (pre): Block(\n",
       "            (module): NextItemPredictionTask(\n",
       "              (item_embedding_table): Embedding(52740, 64, padding_idx=0)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#7.2\n",
    "# Import necessary modules\n",
    "from transformers4rec import torch as tr\n",
    "\n",
    "# Define function: Create input module\n",
    "def create_input_module(schema, max_sequence_length, d_model):\n",
    "    \"\"\"\n",
    "    Create TabularSequenceFeatures input module\n",
    "    \"\"\"\n",
    "    return tr.TabularSequenceFeatures.from_schema(\n",
    "        schema=schema,\n",
    "        max_sequence_length=max_sequence_length,\n",
    "        continuous_projection=64,\n",
    "        aggregation=\"concat\",\n",
    "        d_output=d_model,\n",
    "        masking=\"mlm\"\n",
    "    )\n",
    "\n",
    "# Define function: Create prediction task\n",
    "def create_prediction_task():\n",
    "    \"\"\"\n",
    "    Create next item prediction task\n",
    "    \"\"\"\n",
    "    return tr.NextItemPredictionTask(weight_tying=True)\n",
    "\n",
    "# Define function: Create Transformer configuration\n",
    "def create_transformer_config(d_model, n_head, n_layer, max_sequence_length):\n",
    "    \"\"\"\n",
    "    Create XLNet configuration\n",
    "    \"\"\"\n",
    "    return tr.XLNetConfig.build(\n",
    "        d_model=d_model,\n",
    "        n_head=n_head,\n",
    "        n_layer=n_layer,\n",
    "        total_seq_length=max_sequence_length\n",
    "    )\n",
    "\n",
    "# Define function: Connect modules to build model\n",
    "def build_model(schema, max_sequence_length, d_model, n_head, n_layer):\n",
    "    \"\"\"\n",
    "    Build end-to-end model\n",
    "    \"\"\"\n",
    "    input_module = create_input_module(schema, max_sequence_length, d_model)\n",
    "    prediction_task = create_prediction_task()\n",
    "    transformer_config = create_transformer_config(d_model, n_head, n_layer, max_sequence_length)\n",
    "    return transformer_config.to_torch_model(input_module, prediction_task)\n",
    "\n",
    "# Execute model creation\n",
    "max_sequence_length, d_model, n_head, n_layer = 20, 320, 8, 2\n",
    "model = build_model(schema, max_sequence_length, d_model, n_head, n_layer)\n",
    "\n",
    "# Print model\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb86b811-092b-4dac-b796-f2643dd97dbf",
   "metadata": {},
   "source": [
    "7.3 Define Training Parameters and Initialize Trainer\n",
    "Train the session-based recommendation model using Trainer, with support for half-precision (FP16) acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bdfff07b-03df-486c-be71-bbcccc458dc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T00:34:42.038959Z",
     "iopub.status.busy": "2024-12-02T00:34:42.038229Z",
     "iopub.status.idle": "2024-12-02T00:34:42.082762Z",
     "shell.execute_reply": "2024-12-02T00:34:42.081391Z",
     "shell.execute_reply.started": "2024-12-02T00:34:42.038927Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "#7.3\n",
    "# Import the torch module from Transformers4Rec\n",
    "from transformers4rec import torch as tr\n",
    "import os\n",
    "\n",
    "# Set batch sizes for training and validation\n",
    "BATCH_SIZE_TRAIN = int(os.environ.get(\"BATCH_SIZE_TRAIN\", \"512\"))  # Default training batch size is 512\n",
    "BATCH_SIZE_VALID = int(os.environ.get(\"BATCH_SIZE_VALID\", \"256\"))  # Default validation batch size is 256\n",
    "\n",
    "# Step 1: Define training arguments\n",
    "training_args = tr.trainer.T4RecTrainingArguments(\n",
    "    output_dir=\"./tmp\",  # Path to save training results\n",
    "    max_sequence_length=20,  # Maximum sequence length\n",
    "    data_loader_engine='merlin',  # Data loading engine (default is GPU-optimized Merlin)\n",
    "    num_train_epochs=10,  # Number of training epochs\n",
    "    dataloader_drop_last=False,  # Whether to drop the last batch if it is smaller than batch size\n",
    "    per_device_train_batch_size=BATCH_SIZE_TRAIN,  # Training batch size per device\n",
    "    per_device_eval_batch_size=BATCH_SIZE_VALID,  # Validation batch size per device\n",
    "    learning_rate=0.0005,  # Learning rate\n",
    "    fp16=True,  # Use half-precision (FP16) for accelerated training\n",
    "    report_to=[],  # No external logging systems (e.g., wandb)\n",
    "    logging_steps=200  # Log every 200 steps\n",
    ")\n",
    "\n",
    "# Step 2: Instantiate the trainer\n",
    "recsys_trainer = tr.Trainer(\n",
    "    model=model,  # Use the model defined earlier\n",
    "    args=training_args,  # Training arguments\n",
    "    schema=schema,  # Schema for the model input\n",
    "    compute_metrics=True  # Compute evaluation metrics (e.g., ranking metrics)\n",
    ")\n",
    "\n",
    "# Inform the user that the trainer has been successfully instantiated\n",
    "print(\"Trainer initialized successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab30682-d9cd-4b83-aa96-c90cabbd5d67",
   "metadata": {},
   "source": [
    "7.4 Start Daily Fine-Tuning and Evaluation\n",
    "Use fit_and_evaluate to perform daily fine-tuning and evaluation, \n",
    "supporting time-window-based training loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3631905-8227-46a9-8e84-d604e154809a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T00:38:01.490657Z",
     "iopub.status.busy": "2024-12-02T00:38:01.490287Z",
     "iopub.status.idle": "2024-12-02T00:40:26.109466Z",
     "shell.execute_reply": "2024-12-02T00:40:26.106857Z",
     "shell.execute_reply.started": "2024-12-02T00:38:01.490630Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Launch training for day 178: *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 28672\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 512\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 512\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 560\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='560' max='560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [560/560 00:59, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>7.589800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>6.485400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./tmp/checkpoint-500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='11' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11/11 01:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 20480\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 512\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 512\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Evaluation results for day 179:*****\n",
      "\n",
      " eval_/next-item/avg_precision@10 = 0.08657721430063248\n",
      " eval_/next-item/avg_precision@20 = 0.09137023240327835\n",
      " eval_/next-item/ndcg@10 = 0.11436359584331512\n",
      " eval_/next-item/ndcg@20 = 0.13220059871673584\n",
      " eval_/next-item/recall@10 = 0.2034682035446167\n",
      " eval_/next-item/recall@20 = 0.27398842573165894\n",
      "\n",
      "***** Launch training for day 179: *****\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/400 00:42, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>6.812900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>6.396000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Evaluation results for day 180:*****\n",
      "\n",
      " eval_/next-item/avg_precision@10 = 0.056897178292274475\n",
      " eval_/next-item/avg_precision@20 = 0.06076783314347267\n",
      " eval_/next-item/ndcg@10 = 0.0797809362411499\n",
      " eval_/next-item/ndcg@20 = 0.09389662742614746\n",
      " eval_/next-item/recall@10 = 0.15524475276470184\n",
      " eval_/next-item/recall@20 = 0.21165500581264496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 16896\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 512\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 512\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Launch training for day 180: *****\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='330' max='330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [330/330 00:35, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>6.949600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Evaluation results for day 181:*****\n",
      "\n",
      " eval_/next-item/avg_precision@10 = 0.12699222564697266\n",
      " eval_/next-item/avg_precision@20 = 0.13346321880817413\n",
      " eval_/next-item/ndcg@10 = 0.16788730025291443\n",
      " eval_/next-item/ndcg@20 = 0.1924923062324524\n",
      " eval_/next-item/recall@10 = 0.29870128631591797\n",
      " eval_/next-item/recall@20 = 0.3961038887500763\n",
      "Training and evaluation completed.\n",
      "Results: {'indexed_by_time_eval_/next-item/avg_precision@10': [0.08657721430063248, 0.056897178292274475, 0.12699222564697266], 'indexed_by_time_eval_/next-item/avg_precision@20': [0.09137023240327835, 0.06076783314347267, 0.13346321880817413], 'indexed_by_time_eval_/next-item/ndcg@10': [0.11436359584331512, 0.0797809362411499, 0.16788730025291443], 'indexed_by_time_eval_/next-item/ndcg@20': [0.13220059871673584, 0.09389662742614746, 0.1924923062324524], 'indexed_by_time_eval_/next-item/recall@10': [0.2034682035446167, 0.15524475276470184, 0.29870128631591797], 'indexed_by_time_eval_/next-item/recall@20': [0.27398842573165894, 0.21165500581264496, 0.3961038887500763]}\n"
     ]
    }
   ],
   "source": [
    "#7.4\n",
    "\n",
    "# Import the fit_and_evaluate method\n",
    "from transformers4rec.torch.utils.examples_utils import fit_and_evaluate\n",
    "\n",
    "# Set output directory\n",
    "OUTPUT_DIR = 'preproc_sessions_by_day'\n",
    "\n",
    "# Get time index from environment variables or use default values\n",
    "start_time_idx = int(os.environ.get(\"START_TIME_INDEX\", \"178\"))  # Start time index\n",
    "end_time_idx = int(os.environ.get(\"END_TIME_INDEX\", \"180\"))  # End time index\n",
    "\n",
    "# Use the fit_and_evaluate method to perform fine-tuning and evaluation over the time window\n",
    "OT_results = fit_and_evaluate(\n",
    "    recsys_trainer,  # Instantiated recommendation system trainer\n",
    "    start_time_index=start_time_idx,  # Start time index\n",
    "    end_time_index=end_time_idx,  # End time index\n",
    "    input_dir=OUTPUT_DIR  # Data input directory\n",
    ")\n",
    "\n",
    "# Print training and evaluation results\n",
    "print(\"Training and evaluation completed.\")\n",
    "print(f\"Results: {OT_results}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd28839d-3309-4e29-9936-c37985ea71f3",
   "metadata": {},
   "source": [
    "7.5 Visualize Evaluation Metrics Over Time\n",
    "‘OT_results’ is a list of evaluation scores (e.g., accuracy metrics) \n",
    "calculated based on the given start and end time indices.\n",
    "In this example, we evaluate the data at time indices \n",
    "179, 180, and 181, so OT_results contains metric scores for these three time points.\n",
    "The following code will calculate the average of each metric over time and output the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "143ea26e-831b-47b2-82a9-dec421b98144",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T00:41:09.160484Z",
     "iopub.status.busy": "2024-12-02T00:41:09.160087Z",
     "iopub.status.idle": "2024-12-02T00:41:09.166185Z",
     "shell.execute_reply": "2024-12-02T00:41:09.165023Z",
     "shell.execute_reply.started": "2024-12-02T00:41:09.160442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OT_results: {'indexed_by_time_eval_/next-item/avg_precision@10': [0.08657721430063248, 0.056897178292274475, 0.12699222564697266], 'indexed_by_time_eval_/next-item/avg_precision@20': [0.09137023240327835, 0.06076783314347267, 0.13346321880817413], 'indexed_by_time_eval_/next-item/ndcg@10': [0.11436359584331512, 0.0797809362411499, 0.16788730025291443], 'indexed_by_time_eval_/next-item/ndcg@20': [0.13220059871673584, 0.09389662742614746, 0.1924923062324524], 'indexed_by_time_eval_/next-item/recall@10': [0.2034682035446167, 0.15524475276470184, 0.29870128631591797], 'indexed_by_time_eval_/next-item/recall@20': [0.27398842573165894, 0.21165500581264496, 0.3961038887500763]}\n"
     ]
    }
   ],
   "source": [
    "#7.5 \n",
    "# Import numpy library\n",
    "import numpy as np\n",
    "\n",
    "# Print the contents of OT_results for verification\n",
    "print(\"OT_results:\", OT_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "526450a3-0bcc-44c4-89cf-bf151e37a9b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T01:50:33.218324Z",
     "iopub.status.busy": "2024-11-29T01:50:33.217594Z",
     "iopub.status.idle": "2024-11-29T01:50:33.224117Z",
     "shell.execute_reply": "2024-11-29T01:50:33.223285Z",
     "shell.execute_reply.started": "2024-11-29T01:50:33.218294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " indexed_by_time_eval_/next-item/avg_precision@10 = 0.07962210476398468\n",
      " indexed_by_time_eval_/next-item/avg_precision@20 = 0.08440909907221794\n",
      " indexed_by_time_eval_/next-item/ndcg@10 = 0.1085995187362035\n",
      " indexed_by_time_eval_/next-item/ndcg@20 = 0.12668888767560324\n",
      " indexed_by_time_eval_/next-item/recall@10 = 0.20082703729470572\n",
      " indexed_by_time_eval_/next-item/recall@20 = 0.2726915429035823\n",
      "Average metrics calculation complete.\n"
     ]
    }
   ],
   "source": [
    "#7.6 \n",
    "# Calculate the average metric values over time\n",
    "avg_results = {k: np.mean(v) for k, v in OT_results.items()}\n",
    "\n",
    "# Sort by metric name and print results\n",
    "for key in sorted(avg_results.keys()): \n",
    "    print(\" %s = %s\" % (key, str(avg_results[key])))\n",
    "\n",
    "# Inform that calculation is complete\n",
    "print(\"Average metrics calculation complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a680fb2-c921-4755-aa31-3a14b31d9029",
   "metadata": {},
   "source": [
    "7.7: Model Tracing\n",
    "Convert the model to TorchScript format using torch.jit.trace to support inference and deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "770e6f23-bc84-4f25-9357-2b2edf0b77bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T01:52:50.404192Z",
     "iopub.status.busy": "2024-11-29T01:52:50.403109Z",
     "iopub.status.idle": "2024-11-29T01:52:52.022739Z",
     "shell.execute_reply": "2024-11-29T01:52:52.021713Z",
     "shell.execute_reply.started": "2024-11-29T01:52:50.404167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model tracing completed successfully!\n",
      "Sample input from 'item_id-list__values': tensor([  603,   877,   741,    89,  4776,  1582,  3445,  8082,  3445,  4017,\n",
      "          741,  4776,   183, 12287,  2064,   429,     5,    29,     5,   156,\n",
      "         1986,  2589, 10854,  8216,  4209,  8710,  4241,    80,   111,  4241,\n",
      "         5731,  6809,    32,     5,    72,   663,  2311,  7123,  9112,   444,\n",
      "         1156,   773,   684,   429,  1944,   474,   596,   288,   165,    28,\n",
      "          341,   288,    32,   422,   165,   479,  2771,   287,   961,  4000,\n",
      "         2049,  3273,   498,  1218,   394,  1635, 11838, 10713, 11106,   288,\n",
      "          165,   649,  1084,   301,    87,   649,   213,   303,   176,   316,\n",
      "          422,     5,  3817,   930,  2185,  1084,   205,   686,  3830,   686,\n",
      "          201,    19,    42,    19,  2033, 10456,    19,    20,  1125,  2814,\n",
      "         4209, 34263,   829,   773,   619,  2049,  1986,  1078,  4712,  1335,\n",
      "          660,   288,   429,   862,  4828,  5785, 19155, 17269, 23364,  4208,\n",
      "         3650,  1036,  4769,   223,   276,  1019,   649,   165,  1353,   205,\n",
      "         1888,  2472,  1696,   996,   479,   773,  3840,  4315,  3840,  1229,\n",
      "         3840,  1696,  3808,   474,   980,   803,   312,   612,  1218,  1333,\n",
      "         1940,  2887,  2625,  1333,  2688,   803,   474,   980,   312,   803,\n",
      "         1218,   205,   650,   428,   604,   100,   412,  1964,   626,   813,\n",
      "          626,   813,  4712,  3674,  2788,  3768,  1282,  9539,  1250,   312,\n",
      "          684,  2496,   394,   844,  3461,  2712,  5076,   387,   339,   296,\n",
      "          387,  9640,    45,    60,   821,    60,   601,  2269,   718,  3273,\n",
      "         2555,     1,    60,    23,    95,    60,   422,    60,   474,  3459,\n",
      "         2692,  3459,  3043,  2555,  3987,   991,  1602,   121,  2703,  2786,\n",
      "         3134,   549,   515,    43,  1550,  2701,   205,  1761,    30,   473,\n",
      "          480,   197,   473,  2703,  2392,  1024, 20032,    71,  1333,   223,\n",
      "         3459,  4773,  2049,  6484, 15952,   421,  1487,  2345,  4469,  2547,\n",
      "          570,  1769,  1323,   452,   836,   122,   637,  4758,  3551,  6824,\n",
      "         2739,  5346,  5389,  1168,  4099,  1229,   803,  3587,  2448,   184,\n",
      "           15,   642,   273,   685, 18091, 10456,   608,  2968,  3479,  2968,\n",
      "           36,   608,  2968,   608,    21,  8311,   256,    36,    21,  5360,\n",
      "         7185,  7379,  6051,  7255, 13403,   556,   159,  1663,  4374,  3483,\n",
      "          684,   650,   444,   428,   444,   773,   650,  4283,  1737,  3854,\n",
      "          224,   209,  7244,  6730,   770,  1986,   156,   803,   441,   803,\n",
      "         2090,  1168,  2090,  1168,  3483,  4374,  3483,   444,   428,   429,\n",
      "          422,  1696,  1392,  1797,  2752,   205,  1152, 21587,  2188,  3703,\n",
      "         4462,  5815,  7556,   506,  1796,   813,   626,  2015,   854,  1888,\n",
      "          223,   596,    36,   596, 16532, 10254,     1,  2650,  4027,  2555,\n",
      "         2787,  6378,  1829,  1069,    29,   311,   444,  1084,  1568,  2221,\n",
      "         2663,  1949,  2097,  1671,   223,  4335,   650,   996,  1156,   829,\n",
      "          799,   596,  1084, 12429,   414, 12429,   650,   799,  1755,  1377,\n",
      "         1412,   632,  2033,  7931,  6033,  6359,  4661,   575,  4661,   575,\n",
      "         4661], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#7.7\n",
    "# Import necessary modules\n",
    "# torch: Used for deep learning model inference and tracing.\n",
    "# cudf: For GPU-accelerated DataFrame operations.\n",
    "# merlin and nvtabular: For data manipulation and preprocessing in recommendation systems.\n",
    "import os\n",
    "import torch\n",
    "import cudf\n",
    "from merlin.io import Dataset\n",
    "from nvtabular import Workflow\n",
    "\n",
    "from merlin.table import TensorTable, TorchColumn\n",
    "from merlin.table.conversions import convert_col\n",
    "\n",
    "# Set input data directory and time index\n",
    "# Dynamically read environment variables to specify the start time index and data input directory.\n",
    "# If environment variables are not set, use default values.\n",
    "start_time_idx = int(os.environ.get(\"START_TIME_INDEX\", \"178\"))\n",
    "INPUT_DATA_DIR = os.environ.get(\"INPUT_DATA_DIR\", \"\")\n",
    "\n",
    "# Load processed data as a sample input\n",
    "# Read training data from Parquet files.\n",
    "df = cudf.read_parquet(\n",
    "    os.path.join(INPUT_DATA_DIR, f\"preproc_sessions_by_day/{start_time_idx}/train.parquet\"), \n",
    "    # Load specific columns based on the model's input schema to ensure data matches the model's expected format.\n",
    "    columns=model.input_schema.column_names\n",
    ")\n",
    "\n",
    "# Construct a TensorTable from the DataFrame, selecting the first 100 rows, suitable for small-scale testing.\n",
    "table = TensorTable.from_df(df.iloc[:100])\n",
    "\n",
    "# Set the model's Top-K to 20 to control the number of recommendation candidates.\n",
    "topk = 20\n",
    "model.top_k = topk\n",
    "\n",
    "# Convert the data to TorchColumn format for compatibility with PyTorch models.\n",
    "for column in table.columns:\n",
    "    table[column] = convert_col(table[column], TorchColumn)\n",
    "\n",
    "# Convert the TensorTable to a dictionary, to be used as model input.\n",
    "model_input_dict = table.to_dict()\n",
    "\n",
    "# Switch the model to evaluation mode to disable dropout and other training-specific operations.\n",
    "model.eval()\n",
    "\n",
    "# Perform model inference using the model_input_dict, generating output.\n",
    "output = model(model_input_dict)\n",
    "\n",
    "# Use torch.jit.trace to trace the model and generate an optimized TorchScript representation.\n",
    "# strict=True ensures complete consistency between the inputs and model architecture.\n",
    "traced_model = torch.jit.trace(model, (model_input_dict,), strict=True)\n",
    "\n",
    "# Indicate that tracing is complete.\n",
    "print(\"Model tracing completed successfully!\")\n",
    "\n",
    "# Check the 'item_id-list__values' column in the input dictionary.\n",
    "print(\"Sample input from 'item_id-list__values':\", model_input_dict['item_id-list__values'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9151403d-aced-4830-be1e-293c1df77153",
   "metadata": {},
   "source": [
    "Inspecting Offsets in Input Data\n",
    "\n",
    "In the model input dictionary, the item_id-list__offsets column represents the value offsets, which indicate the range of values corresponding to each example.\n",
    "This is particularly important for handling variable-length sequence data, such as click behavior sequences in user sessions.\n",
    "\n",
    "The following code demonstrates how to inspect the item_id-list__offsets column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71ab4c93-5e66-407e-9ce8-a4b3b86176f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T01:55:04.115551Z",
     "iopub.status.busy": "2024-11-29T01:55:04.114524Z",
     "iopub.status.idle": "2024-11-29T01:55:04.138960Z",
     "shell.execute_reply": "2024-11-29T01:55:04.137655Z",
     "shell.execute_reply.started": "2024-11-29T01:55:04.115519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample offsets from 'item_id-list__offsets': tensor([  0,  12,  14,  16,  19,  21,  23,  26,  32,  35,  45,  47,  49,  51,\n",
      "         56,  59,  61,  66,  69,  72,  76,  78,  80,  83,  87,  91,  94,  96,\n",
      "         98, 100, 102, 104, 107, 111, 113, 122, 124, 128, 130, 133, 136, 141,\n",
      "        143, 161, 164, 167, 172, 176, 180, 182, 184, 191, 193, 196, 199, 201,\n",
      "        209, 214, 234, 237, 243, 245, 265, 267, 274, 276, 281, 289, 295, 298,\n",
      "        300, 305, 307, 312, 314, 317, 320, 324, 327, 331, 335, 337, 339, 343,\n",
      "        345, 348, 351, 354, 356, 360, 362, 364, 367, 374, 376, 381, 383, 386,\n",
      "        388, 396, 401], device='cuda:0', dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Check the 'item_id-list__offsets' column in the input dictionary\n",
    "print(\"Sample offsets from 'item_id-list__offsets':\", model_input_dict['item_id-list__offsets'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c373c10-dbc6-41da-a6e2-8fae5d84cee3",
   "metadata": {},
   "source": [
    "7.8: An inference operation graph is built to integrate feature engineering workflow and PyTorch model into an inference pipeline, and stored in a specified path, which provides the basis for online inference of the recommendation system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b6347a3-e556-4fed-b845-ddc8ead9198f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T01:57:44.565614Z",
     "iopub.status.busy": "2024-11-29T01:57:44.565249Z",
     "iopub.status.idle": "2024-11-29T01:57:45.264970Z",
     "shell.execute_reply": "2024-11-29T01:57:45.263785Z",
     "shell.execute_reply.started": "2024-11-29T01:57:44.565587Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model operation graph created and stored in: /models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 7.8\n",
    "\n",
    "# Import necessary modules\n",
    "# os and shutil: Used for file system operations, such as path management and directory cleanup.\n",
    "# nvtabular.Workflow: Loads the previously saved feature engineering workflow for preprocessing input data.\n",
    "# PredictPyTorch and TransformWorkflow:\n",
    "# PredictPyTorch: Responsible for invoking the TorchScript model for predictions.\n",
    "# TransformWorkflow: Applies the feature engineering workflow.\n",
    "import os\n",
    "import shutil\n",
    "from nvtabular import Workflow\n",
    "from merlin.systems.dag.ops.pytorch import PredictPyTorch\n",
    "from merlin.systems.dag.ops.workflow import TransformWorkflow\n",
    "\n",
    "# Set the model storage path\n",
    "# Dynamically retrieve the model storage path using the environment variable `ens_model_path`\n",
    "# or use the default path: {INPUT_DATA_DIR}/models\n",
    "ens_model_path = os.environ.get(\"ens_model_path\", f\"{INPUT_DATA_DIR}/models\")\n",
    "\n",
    "# Clean up the existing directory and create a new folder\n",
    "# Ensure the model storage path is a clean directory to avoid confusion with old files.\n",
    "if os.path.isdir(ens_model_path):\n",
    "    shutil.rmtree(ens_model_path)  # Delete the existing directory\n",
    "os.mkdir(ens_model_path)  # Create a new directory\n",
    "\n",
    "# Load the previously saved workflow\n",
    "# A workflow defines feature transformations and records the logic of feature engineering.\n",
    "workflow = Workflow.load(os.path.join(INPUT_DATA_DIR, \"workflow_etl\"))\n",
    "\n",
    "# Build the operation graph with workflow and TorchScript model\n",
    "torch_op = (\n",
    "    workflow.input_schema.column_names  # Input features\n",
    "    >> TransformWorkflow(workflow)  # Apply the workflow for feature transformations\n",
    "    >> PredictPyTorch(traced_model, model.input_schema, model.output_schema)  # Use the PyTorch model for predictions\n",
    ")\n",
    "\n",
    "# Indicate completion of the operation\n",
    "print(f\"Model operation graph created and stored in: {ens_model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2993848",
   "metadata": {},
   "source": [
    "# 7.8: Saving Model parameter for attack model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cd9d6d",
   "metadata": {},
   "source": [
    "df = cudf.read_parquet(os.path.join(INPUT_DATA_DIR, f\"preproc_sessions_by_day/{start_time_idx}/train.parquet\"), columns=model.input_schema.column_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856e6404",
   "metadata": {},
   "source": [
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>product_recency_days_log_norm-list</th>\n",
    "      <th>et_dayofweek_sin-list</th>\n",
    "      <th>item_id-list</th>\n",
    "      <th>category-list</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>[-2.751772, -3.0140762, -3.0140762, -2.7857492...</td>\n",
    "      <td>[0.9749277124471076, 0.9749277124471076, 0.974...</td>\n",
    "      <td>[18299, 16220, 15865, 10992, 9671, 10416, 2192...</td>\n",
    "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>[-2.6465325, -2.6465726, -2.6463258, -2.646005...</td>\n",
    "      <td>[0.9749277124471076, 0.9749277124471076, 0.974...</td>\n",
    "      <td>[370, 650, 650, 370, 273, 339, 191, 2340, 1758...</td>\n",
    "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>[-2.6599658, -2.6397069, -2.6337621, -2.638489...</td>\n",
    "      <td>[0.9749277124471076, 0.9749277124471076, 0.974...</td>\n",
    "      <td>[740, 18, 766, 18, 531, 531, 18, 740, 18, 150,...</td>\n",
    "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>[-2.55788, -2.572989, -2.5622084, -2.5649703, ...</td>\n",
    "      <td>[0.9749277124471076, 0.9749277124471076, 0.974...</td>\n",
    "      <td>[858, 4392, 38, 90, 1034, 2654, 594, 7102, 345...</td>\n",
    "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>5</th>\n",
    "      <td>[-2.6032808, -2.601593, -2.6026015, -2.6122754...</td>\n",
    "      <td>[0.9749277124471076, 0.9749277124471076, 0.974...</td>\n",
    "      <td>[113, 74, 113, 132, 113, 113, 132, 113, 3206, ...</td>\n",
    "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68bbcd2-e01c-42d7-8072-c0c43d2709eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_seq = 10\n",
    "table = TensorTable.from_df(df.iloc[:row_seq])\n",
    "for column in table.columns:\n",
    "    table[column] = convert_col(table[column], TorchColumn)\n",
    "model_input_dict = table.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f25a67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "item_scores = model(model_input_dict)\n",
    "item_scores, len(item_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f772abcd",
   "metadata": {},
   "source": [
    "(tensor([[ 0.3122, -0.1045, -0.0541,  ...,  0.3079,  0.1996, -0.0404],\n",
    "         [ 0.3153, -0.1078, -0.0513,  ...,  0.3066,  0.2012, -0.0439],\n",
    "         [ 0.3490, -0.1086, -0.0560,  ...,  0.2799,  0.2165, -0.0544],\n",
    "         ...,\n",
    "         [ 0.3157, -0.1130, -0.0496,  ...,  0.3101,  0.2063, -0.0433],\n",
    "         [ 0.3613, -0.0942, -0.0499,  ...,  0.2601,  0.2228, -0.0615],\n",
    "         [ 0.3429, -0.0999, -0.0530,  ...,  0.2790,  0.2091, -0.0528]],\n",
    "        device='cuda:0', grad_fn=<DivBackward0>),\n",
    " 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9a2ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(item_scores, f\"mia/output_member_{row_seq}_seqs.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9dff9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b5d058",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = cudf.read_parquet(os.path.join(INPUT_DATA_DIR, f\"preproc_sessions_by_day/{start_time_idx}/test.parquet\"), columns=model.input_schema.column_names)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a17f97",
   "metadata": {},
   "source": [
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>product_recency_days_log_norm-list</th>\n",
    "      <th>et_dayofweek_sin-list</th>\n",
    "      <th>item_id-list</th>\n",
    "      <th>category-list</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>[-2.5880072, -2.5885816, -2.5964737, -2.587951...</td>\n",
    "      <td>[0.9749277124471076, 0.9749277124471076, 0.974...</td>\n",
    "      <td>[671, 1757, 1296, 1757, 1296, 1296, 1296, 233,...</td>\n",
    "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>14</th>\n",
    "      <td>[-2.8195918, -2.8189173, -2.8187954, -2.814405...</td>\n",
    "      <td>[0.9749277124471076, 0.9749277124471076, 0.974...</td>\n",
    "      <td>[715, 1498, 1498, 550, 339, 2219, 2775, 2689, ...</td>\n",
    "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>21</th>\n",
    "      <td>[-2.6729205, -2.6728654, -2.7348363, -2.726106...</td>\n",
    "      <td>[0.9749277124471076, 0.9749277124471076, 0.974...</td>\n",
    "      <td>[433, 433, 1227, 1786, 1786, 1910, 3418, 3418,...</td>\n",
    "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>30</th>\n",
    "      <td>[-2.7185333, -2.7196856, -2.7212174, -2.721435...</td>\n",
    "      <td>[0.9749277124471076, 0.9749277124471076, 0.974...</td>\n",
    "      <td>[1202, 2168, 715, 1922, 1498, 468, 3059, 2168,...</td>\n",
    "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>39</th>\n",
    "      <td>[-2.611448, -2.6098468, -2.609604, -2.6047618,...</td>\n",
    "      <td>[0.9749277124471076, 0.9749277124471076, 0.974...</td>\n",
    "      <td>[1655, 1655, 1655, 12942, 12035, 4820, 7783, 3...</td>\n",
    "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9cfb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_seq = 10\n",
    "table = TensorTable.from_df(df_test.iloc[:row_seq])\n",
    "for column in table.columns:\n",
    "    table[column] = convert_col(table[column], TorchColumn)\n",
    "model_input_dict_test = table.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a24ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "item_scores_test = model(model_input_dict_test)\n",
    "item_scores_test, len(item_scores_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0a4c81",
   "metadata": {},
   "source": [
    "(tensor([[ 0.2834, -0.1043, -0.0532,  ...,  0.3190,  0.1886, -0.0442],\n",
    "         [ 0.3393, -0.1000, -0.0495,  ...,  0.2840,  0.2094, -0.0501],\n",
    "         [ 0.3394, -0.1019, -0.0567,  ...,  0.2746,  0.2147, -0.0532],\n",
    "         ...,\n",
    "         [ 0.3410, -0.1085, -0.0532,  ...,  0.2819,  0.2083, -0.0525],\n",
    "         [ 0.3723, -0.0842, -0.0578,  ...,  0.2486,  0.2149, -0.0606],\n",
    "         [ 0.3594, -0.0950, -0.0568,  ...,  0.2632,  0.2167, -0.0586]],\n",
    "        device='cuda:0', grad_fn=<DivBackward0>),\n",
    " 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf2a106",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(item_scores_test, f\"mia/output_nonmember_{row_seq}_seqs.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
